# ğŸ§  PyCode-TextEE  
> Efficient and extensible Event Extraction with prompt optimization â€” built on top of [TextEE](https://github.com/ej0cl6/TextEE)

---

<div align="center">

[![ğŸ› ï¸ Updates](https://img.shields.io/badge/ğŸ› ï¸%20Updates-Click%20Here-informational?style=flat-square)](#ï¸-updates) â€¢
[![ğŸ“‚ Datasets](https://img.shields.io/badge/ğŸ“‚%20Datasets-17%20Supported-brightgreen?style=flat-square)](#ï¸-datasets) â€¢
[![âš™ï¸ Environment](https://img.shields.io/badge/âš™ï¸%20Environment-Setup-orange?style=flat-square)](#ï¸-environment) â€¢
[![ğŸš€ Running](https://img.shields.io/badge/ğŸš€%20Running-Instructions-blueviolet?style=flat-square)](#ï¸-running) â€¢
[![ğŸŒ Website](https://img.shields.io/badge/ğŸŒ%20Website-Demo%20Soon-lightgrey?style=flat-square)](#ï¸-website) â€¢
[![ğŸ“„ Paper](https://img.shields.io/badge/ğŸ“„%20arXiv-2502.16377-b31b1b?style=flat-square)](https://arxiv.org/abs/2502.16377)

</div>

---

**Authors**:  
Saurabh Srivastava, Sweta Pati, Ziyu Yao

---

## ğŸ§© Introduction

**PyCode-TextEE** extends [TextEE](https://github.com/ej0cl6/TextEE), bringing **event extraction into the era of prompt-based large language models**.

While **TextEE** standardizes 10+ event extraction datasets into a unified JSON formatâ€”making them reproducible and comparableâ€”**PyCode-TextEE** takes the next leap:

> âœ¨ We transform TextEE-formatted data into **code-style prompts**â€”a format that is both readable and executable by LLMs and ideal for structured evaluation.

---

### ğŸš€ Whatâ€™s New in PyCode-TextEE?

- ğŸ§± **CodePrompt Format Conversion**  
  We convert event structures (event triggers, argumentsâ€”if available) into Python-like prompts (e.g., `Attack(mention="...", attacker=[...], target=[...])`) to help LLMs handle structured outputs.

- ğŸ“¦ **Plug-and-Play with TextEE**  
  Directly load standardized datasets from TextEE and transform them with one command into training-ready CodePrompts.

- ğŸ§ª **Evaluation Toolkit for Prompted LLMs**  
  We provide exact-match evaluation utilities that compute **precision, recall, and F1 scores** over structured LLM outputs.

- ğŸ“ **Upcoming Paper-Compatible Training Pipelines**  
  Includes all data transformations and training scripts used in a paper currently under submission to a major NLP conference. Code for that will live in `LLaMAEvents/`.

## ğŸ› ï¸ Updates

- **April 23, 2025** â€” We release **PyCode-TextEE**, a modular framework for converting standardized event extraction datasets (via TextEE) into code-style prompts, along with exact-match evaluation scripts.  
  Feel free to reach out if youâ€™d like to contribute your **models**, **datasets**, or ideas!

## ğŸ“‚ Supported Datasets

We support **17 datasets** for Event Detection (ED), Event Argument Extraction (EAE), and End-to-End (E2E) Event Extraction.  
All are converted into **code-style prompts** and support evaluation using our exact-match metric suite.  
The table below also shows whether annotation **guidelines** are included for each dataset.

<div align="center">
<table>
<thead>
<tr>
  <th><strong>Dataset</strong></th>
  <th><strong>Task(s)</strong></th>
  <th><strong>Paper Title</strong></th>
  <th><strong>Source</strong></th>
  <th><strong>Guidelines</strong></th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>ACE05</code></td>
  <td>ED, EAE, E2E</td>
  <td>The Automatic Content Extraction (ACE) Program</td>
  <td><a href="https://www.ldc.upenn.edu/">LDC</a></td>
  <td>ğŸ”˜</td>
</tr>
<tr>
  <td><code>ERE</code></td>
  <td>ED, EAE, E2E</td>
  <td>From Light to Rich ERE</td>
  <td><a href="https://www.ldc.upenn.edu/">LDC</a></td>
  <td>ğŸ”˜</td>
</tr>
<tr>
  <td><code>MLEE</code></td>
  <td>ED, EAE, E2E</td>
  <td>Biological Event Extraction</td>
  <td><a href="https://academic.oup.com/bioinformatics/article/28/18/i575/245077">Bioinformatics</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>Genia2011</code></td>
  <td>ED, EAE, E2E</td>
  <td>Genia Event Task (2011)</td>
  <td><a href="https://www.aclweb.org/anthology/W11-1801/">BioNLP 2011</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>Genia2013</code></td>
  <td>ED, EAE, E2E</td>
  <td>Genia Event Task (2013)</td>
  <td><a href="https://www.aclweb.org/anthology/W13-5201/">BioNLP 2013</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>M2E2</code></td>
  <td>ED, EAE, E2E</td>
  <td>Cross-media Structured Common Space</td>
  <td><a href="https://aclanthology.org/2020.acl-main.188/">ACL 2020</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>CASIE</code></td>
  <td>ED, EAE, E2E</td>
  <td>CASIE: Cybersecurity Event Extraction</td>
  <td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6155">AAAI 2020</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>PHEE</code></td>
  <td>ED, EAE, E2E</td>
  <td>Pharmacovigilance Event Extraction</td>
  <td><a href="https://aclanthology.org/2022.emnlp-main.343/">EMNLP 2022</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>MEE</code></td>
  <td>ED</td>
  <td>Multilingual Event Extraction</td>
  <td><a href="https://aclanthology.org/2022.emnlp-main.428/">EMNLP 2022</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>FewEvent</code></td>
  <td>ED</td>
  <td>Few-Shot Event Detection</td>
  <td><a href="https://dl.acm.org/doi/10.1145/3336191.3371962">WSDM 2020</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>MAVEN</code></td>
  <td>ED</td>
  <td>Massive General-Domain ED</td>
  <td><a href="https://aclanthology.org/2020.emnlp-main.154/">EMNLP 2020</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>SPPED</code></td>
  <td>ED</td>
  <td>ED from Social Media for Epidemic Prediction</td>
  <td><a href="https://aclanthology.org/2024.naacl-main.172/">NAACL 2024</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>MUC-4</code></td>
  <td>EAE</td>
  <td>Fourth Message Understanding Conference</td>
  <td><a href="https://www-nlpir.nist.gov/related_projects/muc/">MUC 1992</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>RAMS</code></td>
  <td>EAE</td>
  <td>Multi-Sentence Argument Linking</td>
  <td><a href="https://aclanthology.org/2020.acl-main.743/">ACL 2020</a></td>
  <td>ğŸ”˜</td>
</tr>
<tr>
  <td><code>WikiEvents</code></td>
  <td>EAE</td>
  <td>Conditional Generation for Doc-level EAE</td>
  <td><a href="https://aclanthology.org/2021.naacl-main.417/">NAACL 2021</a></td>
  <td>ğŸ”˜</td>
</tr>
<tr>
  <td><code>GENEVA</code></td>
  <td>EAE</td>
  <td>Benchmarking Generalizability for EAE</td>
  <td><a href="https://aclanthology.org/2023.acl-long.794/">ACL 2023</a></td>
  <td>ğŸ”˜</td>
</tr>
</tbody>
</table>


</div>

## âš™ï¸ Environment

We recommend using **Python 3.9+** with a clean virtual environment (e.g., via `venv` or `conda`).

### ğŸ”¹ Install Dependencies
```bash
# Clone the repo
git clone https://github.com/yourname/PyCode-TextEE.git
cd PyCode-TextEE

# Create a virtual environment (optional)
python3 -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate

# Install requirements
pip install -r requirements.txt
```

### ğŸ”¹ Core Dependencies
- `transformers` â‰¥ 4.x  
- `datasets`  
- `openai`  
- `scikit-learn`  
- `tqdm`  
- `wandb` (optional for experiment tracking)  

### âš ï¸ Note
Some datasets (e.g., ACE, ERE) require **LDC license** to access raw files.  We provide code for preprocessing them, but not the data itself.


## ğŸš€ Running the Code

Below is a step-by-step guide to run PyCode-TextEE.  
Our pipeline is divided into two main stages:

---

### ğŸŒ± Step 1 â€” [Optional] Generate Code Schema

If you're working with custom datasets (or want to regenerate schemas for the 17 supported ones), you'll first convert them into **TextEE format** and generate the corresponding **Python-style event definitions**.

ğŸ“ Directory structure:
```
PyCode-TextEE/
â”œâ”€â”€ code_schema_generation/
â”‚   â”œâ”€â”€ generate_schema.py
â”‚   â”œâ”€â”€ init_prompts/            # Contains per-dataset class schemas (*.txt)
â”‚   â”œâ”€â”€ python_event_defs/       # Python classes for eval (dataset-wise + all_ee_definitions.py)
â”‚   â”œâ”€â”€ mapper.json              # Maps cleaned names â†’ class names
â”‚   â””â”€â”€ schema.json              # All cleaned event/arg schemas
```

ğŸ‘¾ Example output schema (for ACE05 `Attack` event):
```python
@dataclass
class Attack(ConflictEvent):
    mention: str
    target: List
    victim: List
    attacker: List
    instrument: List
    place: List
    agent: List
```

ğŸ›  To generate schema:
```bash
cd code_schema_generation
python generate_schema.py --dataset_folder <your_dataset_dir>
```

ğŸ“‚ Expected dataset layout:
```
<your_dataset_dir>/
â”œâ”€â”€ ace05-en/
â”‚   â”œâ”€â”€ split1/
â”‚   â”‚   â”œâ”€â”€ train.json
â”‚   â”‚   â”œâ”€â”€ dev.json
â”‚   â”‚   â””â”€â”€ test.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ casie/
â””â”€â”€ ...
```

**Note**: Weâ€™ve already generated schema for all 17 supported datasets. This step is only required for new datasets.

---

### ğŸ”¹ Step 2: Convert to Code Prompts

Run the following:

```bash
cd code_prompts
python prepare_dataset.py \
    --input_dir ../../TextEE/processed_data \
    --dataset_name ace05-en \
    --annotate_schema True \
    --guideline_file ace05-en \
    --add_negative_samples True \
    --output_dir ./processed_code_prompts/
```

---

### âš™ï¸ Argument Descriptions

| Argument               | Description |
|------------------------|-------------|
| `--input_dir`          | Path to TextEE-formatted JSONs (default: `../../TextEE/processed_data`) |
| `--dataset_name`       | Name of the dataset to process (e.g., `ace05-en`) |
| `--annotate_schema`    | Add class docstrings and inline comments using guidelines (default: `False`) |
| `--guideline_file`     | Guideline JSON file for schema annotation (required if `annotate_schema=True`) |
| `--add_negative_samples` | Add negative examples to training set (default: `False`) |
| `--output_dir`         | Where to save the converted code prompts (default: `./processed_code_prompts/`) |

---

### ğŸ§¬ Annotated Prompt Example (with Guidelines)

When `--annotate_schema=True`, we generate prompts like:

```python
@dataclass
class Die(LifeEvent):
    """the event def"""
    mention: str  # ... definition of argument
    place: List   # ... definition of argument
    instrument: List  # ... definition of argument
    victim: List  # ... definition of argument
    person: List  # ... definition of argument
    agent: List   # ... definition of argument
```

This format supports **LLM-compatible** structure learning and improves interpretability.

---

### ğŸ“˜ Guideline File Format

Your `guideline_file` should look like:

```json
{
  "EventName1": {
    "description": [
      "One possible definition.",
      "Another variation of the same."
    ],
    "attributes": {
      "mention": "Trigger span of the event.",
      "agent": "Entity responsible for the event."
    }
  }
}
```

This enables *randomized sampling* during conversion to avoid overfitting to one phrasingâ€”an approach highlighted in our arXiv/ACL paper.

---

### ğŸ’¡ Tip
â“µ Skip `--guideline_file` and `--annotate_schema` if you're only interested in raw code prompts.  
â“¶ Use `--add_negative_samples` if you want to add negative sample per instance similar to DEGREE.




